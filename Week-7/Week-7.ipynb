{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Week 7\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "- [Capstone Objectives](#Capstone-Objectives)\n",
    "- [Read in Data](#Read-in-Data)\n",
    "    - [Merge 2018 and 2019](#Merge-2018-and-2019)\n",
    "    - [Make advisor dictionary mapper](#Make-advisor-dictionary-mapper)\n",
    "- [Data Cleaning](#Data-Cleaning)\n",
    "    - [Train-Test-Split](#Train-Test-Split)\n",
    "    - [Custom Cleaning Functions](#Custom-Cleaning-Functions)\n",
    "    - [Create Cleaning Pipeline](#Create-Cleaning-Pipeline)\n",
    "- [Model building](#Model-building)\n",
    "- [Make predictions](#Make-predictions)\n",
    "    - [Regression](#Regression)\n",
    "        - [Make Function to output deciles](#Make-Function-to-output-deciles)\n",
    "    - [Classification](#Classification)\n",
    "        - [Balance the data with `imbalanced-learn`](#Balance-the-data-with-imbalanced-learn)\n",
    "    - [Model Interpretation](#Model-Interpretation)\n",
    "- [Scratch Work](#Scratch-Work)\n",
    "    - [Feature Engineering](#Feature-Engineering)\n",
    "        - [Variable Inflation Factor (VIF)](#Variable-Inflation-Factor-(VIF))\n",
    "    - [Residuals](#Residuals)\n",
    "    - [Condition number](#Condition-number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Objectives\n",
    "- Assist sales and marketing by improving their targeting\n",
    "- Predict sales for 2019 using the data for 2018\n",
    "- Estimate the probability of adding a new fund in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df18 = pd.read_excel(\"../Transaction Data.xlsx\", sheet_name=\"Transactions18\")\n",
    "df19 = pd.read_excel(\"../Transaction Data.xlsx\", sheet_name=\"Transactions19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge 2018 and 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    df18, \n",
    "    df19, \n",
    "    on='CONTACT_ID',\n",
    "    suffixes=['_2018', '_2019']\n",
    ")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make advisor dictionary mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adviser_lookup = {idx: contact_id for idx, contact_id in enumerate(df['CONTACT_ID'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adviser_lookup[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a variable to keep all of the columns we want to drop\n",
    "COLS_TO_DROP = [\n",
    "    'refresh_date_2019', 'refresh_date_2018', 'CONTACT_ID', \n",
    "]\n",
    "\n",
    "COLS_TO_KEEP = [\n",
    "    'no_of_sales_12M_1', 'no_of_Redemption_12M_1', 'no_of_sales_12M_10K',\n",
    "    'no_of_Redemption_12M_10K', 'no_of_funds_sold_12M_1',\n",
    "    'no_of_funds_redeemed_12M_1', 'no_of_fund_sales_12M_10K',\n",
    "    'no_of_funds_Redemption_12M_10K', 'no_of_assetclass_sold_12M_1',\n",
    "    'no_of_assetclass_redeemed_12M_1', 'no_of_assetclass_sales_12M_10K',\n",
    "    'no_of_assetclass_Redemption_12M_10K', 'No_of_fund_curr',\n",
    "    'No_of_asset_curr', 'AUM', 'sales_curr', 'sales_12M_2018',\n",
    "    'redemption_curr', 'redemption_12M', 'new_Fund_added_12M_2018',\n",
    "    'aum_AC_EQUITY', 'aum_AC_FIXED_INCOME_MUNI',\n",
    "    'aum_AC_FIXED_INCOME_TAXABLE', 'aum_AC_MONEY', 'aum_AC_MULTIPLE',\n",
    "    'aum_AC_PHYSICAL_COMMODITY', 'aum_AC_REAL_ESTATE', 'aum_AC_TARGET',\n",
    "    'aum_P_529', 'aum_P_ALT', 'aum_P_CEF', 'aum_P_ETF', 'aum_P_MF',\n",
    "    'aum_P_SMA', 'aum_P_UCITS', 'aum_P_UIT'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['sales_12M_2019', 'new_Fund_added_12M_2019'], axis=1)\n",
    "y_reg = df['sales_12M_2019']\n",
    "y_cl = df['new_Fund_added_12M_2019']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reg.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_reg, y_test_reg = train_test_split(\n",
    "    X, y_reg, test_size=0.25, random_state=24, stratify=y_reg.isnull())\n",
    "y_train_cl, y_test_cl = y_cl[y_train_reg.index], y_cl[y_test_reg.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reg.isnull().value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_reg.isnull().value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns(df):\n",
    "    '''extract out columns not listed in COLS_TO_DROP variable'''\n",
    "    cols_to_keep = [col for col in df.columns if col not in COLS_TO_DROP]\n",
    "    return df.loc[:, cols_to_keep].copy()\n",
    "\n",
    "def fillna_values(df):\n",
    "    '''fill nan values with zero'''\n",
    "    if isinstance(df, type(pd.Series(dtype='float64'))):\n",
    "        return df.fillna(0)\n",
    "    elif isinstance(df, type(pd.DataFrame())):\n",
    "        num_df = df.select_dtypes(include=['number']).fillna(0)\n",
    "        non_num_df = df.select_dtypes(exclude=['number'])\n",
    "        return pd.concat([num_df, non_num_df], axis=1)\n",
    "    else:\n",
    "        return np.nan_to_num(df)\n",
    "\n",
    "def negative_to_zero(series):\n",
    "    '''fill negative values to zero'''\n",
    "    if isinstance(series, type(pd.Series(dtype='float64'))):\n",
    "        return series.apply(lambda x: max(0, x))\n",
    "    else:\n",
    "        return series\n",
    "    \n",
    "def bin_y_class(series):\n",
    "    series = series.apply(lambda x: 1 if x >=1 else 0)\n",
    "    return series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "## Create Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert functions to transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_columns_trans = FunctionTransformer(extract_columns)\n",
    "fillna_values_trans = FunctionTransformer(fillna_values)\n",
    "negative_to_zero_trans = FunctionTransformer(negative_to_zero)\n",
    "bin_y_class_trans = FunctionTransformer(bin_y_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make pipeline for target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_targ_pipe = Pipeline([\n",
    "    ('fillna_values_trans', fillna_values_trans),\n",
    "    ('negative_to_zero', negative_to_zero_trans),\n",
    "])\n",
    "\n",
    "class_targ_pipe = Pipeline([\n",
    "    ('fillna_values_trans', fillna_values_trans),\n",
    "    ('bin_y_class_trans', bin_y_class_trans),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and transform TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reg = reg_targ_pipe.fit_transform(y_train_reg)\n",
    "y_train_cl = class_targ_pipe.fit_transform(y_train_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform only TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_reg = reg_targ_pipe.transform(y_test_reg) \n",
    "y_test_cl = class_targ_pipe.transform(y_test_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cl.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make pipeline for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit_transform(X_train[['no_of_sales_12M_1', 'no_of_Redemption_12M_1']].fillna(0), y_train_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.transform(X_train[['no_of_sales_12M_1', 'no_of_Redemption_12M_1']].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.transform(X_test[['no_of_sales_12M_1', 'no_of_Redemption_12M_1']].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_pipe = Pipeline([\n",
    "    ('extract_columns_trans', extract_columns_trans),\n",
    "    ('fillna_values_trans', fillna_values_trans),\n",
    "    ('StandardScaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# fit and transform TRAINING\n",
    "train_array = feat_pipe.fit(X_train, y_train_reg).transform(X_train)\n",
    "\n",
    "# Give training data row and column labels back\n",
    "X_train_prepared = pd.DataFrame(\n",
    "    train_array,\n",
    "    index=X_train.index,\n",
    "    columns=COLS_TO_KEEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prepared.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRANSFORM** the test set (Do NOT fit the pipeline on testing!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prepared = pd.DataFrame(\n",
    "    feat_pipe.transform(X_test),\n",
    "    index=X_test.index,\n",
    "    columns=COLS_TO_KEEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prepared.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reg.hist(bins=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(np.log(y_train_reg+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.compose import TransformedTargetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_log(x):\n",
    "    x = np.where(x<0, 0, x)\n",
    "    return np.log(x, where=x>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_exp(x):\n",
    "    x = np.where(x>15, 15, x)\n",
    "    return np.expm1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttr = TransformedTargetRegressor(\n",
    "#         RidgeCV(), \n",
    "#         func=modified_log, \n",
    "#         inverse_func=modified_exp, \n",
    "#         check_inverse=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr = TransformedTargetRegressor(\n",
    "        RidgeCV(), \n",
    "        func=np.log1p, \n",
    "        inverse_func=modified_exp, \n",
    "        check_inverse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr.fit(X_train_prepared, y_train_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "# Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = pd.Series(ttr.predict(X_test_prepared), index=y_test_reg.index)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr.score(X_test_prepared, y_test_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "### Make Function to output deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_deciles(model, X, y):\n",
    "    results = pd.DataFrame(model.predict(X), index=y.index, columns=['prediction'])\n",
    "    results['actual'] = y.values\n",
    "    results['deciles'] = pd.qcut(results['prediction'], 10, labels=False)\n",
    "    results['contact_id'] = results.index.map(adviser_lookup)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = output_deciles(ttr, X_test_prepared, y_test_reg)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.drop(columns='contact_id').groupby('deciles').sum().sort_index(ascending=False).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cl.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cl.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_base = GradientBoostingClassifier()\n",
    "gbt_base.fit(X_train_prepared, y_train_cl)\n",
    "test_pred_class = gbt_base.predict(X_test_prepared)\n",
    "print(classification_report(y_test_cl, test_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_deciles_class(model, X, y):\n",
    "    results = pd.DataFrame(model.predict_proba(X)[:,1], index=y.index, columns=['prediction'])\n",
    "    results['actual'] = y.values\n",
    "    results['deciles'] = pd.qcut(results['prediction'], 10, labels=False)\n",
    "    results['contact_id'] = results.index.map(adviser_lookup)\n",
    "    return results.sort_values(by='prediction', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_deciles_class(gbt_base, X_train_prepared, y_train_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the data with `imbalanced-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prepared.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cl.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate SMOTENC\n",
    "smote = SMOTE(random_state=0)\n",
    "\n",
    "# balance data\n",
    "X_smote, y_smote = smote.fit_resample(X_train_prepared, y_train_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_smote.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_base2 = GradientBoostingClassifier()\n",
    "gbt_base2.fit(X_smote, y_smote)\n",
    "test_pred_class2 = gbt_base2.predict(X_test_prepared)\n",
    "print(classification_report(y_test_cl, test_pred_class2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "# Model Interpretation\n",
    "\n",
    "Using Shapley Values to Interpret Models: see [`shap`](https://github.com/slundberg/shap) package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install shap\n",
    "# or\n",
    "# !conda install -yc conda-forge shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize explainer\n",
    "explainer = shap.LinearExplainer(ttr.regressor_, X_train_prepared)\n",
    "\n",
    "# get shapley values using your data (like .fit method in sklearn)\n",
    "shap_values = explainer.shap_values(X_test_prepared)\n",
    "\n",
    "# visualize the first prediction's explaniation\n",
    "shap.force_plot(explainer.expected_value, shap_values[0, :], X_test_prepared.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base value is mean of all training prediction\n",
    "np.mean(ttr.regressor_.predict(X_train_prepared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prediction for this advisor is the bold number\n",
    "ttr.regressor_.predict(X_test_prepared.iloc[0, :].values.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the numbers next to the features are the values for each feature\n",
    "X_test_prepared.loc[6307, 'no_of_assetclass_sold_12M_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The low value in 'no_of_assetclass_sold_12M_1' pushes the prediction down\n",
    "X_test_prepared['no_of_assetclass_sold_12M_1'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the effects of all the features\n",
    "shap.summary_plot(shap_values, X_test_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows an overview of which features are most important for a model. The first features have the higest shapley value magnitudes over all samples. The colors represent the feature value. For example, a **high** number of `no_of_assetclass_sold_12M_1` **increases** the sales amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1000\n",
    "sample = X_test_prepared.sample(k, random_state=0)\n",
    "shap_sample = pd.DataFrame(shap_values, index=X_test_prepared.index).loc[sample.index, :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING! This can be slow...\n",
    "shap.force_plot(explainer.expected_value, shap_sample, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the top decile\n",
    "reg_results_df = output_deciles(ttr, X_test_prepared, y_test_reg)\n",
    "reg_results_df[reg_results_df['deciles'] == 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only rows in testing from top decile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_decile_test_idx = reg_results_df[reg_results_df['deciles'] == 9].index\n",
    "top_decile_test = X_test_prepared.loc[top_decile_test_idx, :]\n",
    "top_decile_shap = pd.DataFrame(shap_values, index=X_test_prepared.index).loc[top_decile_test_idx, :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, top_decile_shap, top_decile_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(top_decile_shap, top_decile_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "# Scratch Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lift Chart Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -yc conda-forge scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_proba = gbt_base2.predict_proba(X_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_lift_curve(y_test_cl, test_pred_proba);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-cross_validate(feat_pipe, X_train_prepared, y_train_reg, scoring='neg_root_mean_squared_error')['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    print(\"Cross Validation Scores:\")\n",
    "    print(cross_validate(model, X, y, scoring='neg_root_mean_squared_error')['test_score'])\n",
    "    print('-'*55)\n",
    "    preds = np.exp(model.predict(X))\n",
    "    lim = max(preds.max(), y.max())\n",
    "    fig, ax = plt.subplots(1,1,figsize=(7,5))\n",
    "    ax.scatter(x=y, y=preds, alpha=0.4)\n",
    "    ax.plot([0, 10000], [0, 10000])\n",
    "    ax.set_xlim([0, 10000])\n",
    "    ax.set_ylim([0, 10000])\n",
    "    ax.set_title(\"Actual vs Predicted - Regression\")\n",
    "    ax.set_xlabel(\"Actual\")\n",
    "    ax.set_ylabel(\"Predicted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(feat_pipe, X_train_prepared, y_train_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reg_log = np.log(y_train_reg+1)\n",
    "y_test_reg_log = np.log(y_test_reg+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_pipe.fit(X_train_prepared, y_train_reg_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_pipe.predict(X_test_prepared)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(1.5966611)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(feat_pipe, X_train, y_train_reg_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(feat_pipe, X_test, y_test_reg_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is feature engineering**?\n",
    "\n",
    "\"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering.\" - Andrew Ng\n",
    "\n",
    "Feature engineering is the term broadly applied to the creation and manipulation of features (variables) used in machine learning algorithms. Unless we're working with the same data over and over again, this isn't something we can automate. It will require creativity and a good, thorough understanding of our data.\n",
    "\n",
    "Regression results can change significantly depending on feature selection. Let's take a closer look at our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prepared.corr().style.background_gradient().set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prepared.hist(bins=40, figsize=(16,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "### Variable Inflation Factor (VIF)\n",
    "\n",
    "VIF measures the amount of multicollinearity in a set of multiple regressors, by evaluating how much the variance of the independent variable is inflated by it's interaction with other independent variables. VIF threshold of 5 to 10 are acceptable, but values above 10 are too high.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class ReduceVIF(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, thresh=10.0, impute=True, impute_strategy='median'):\n",
    "        # From looking at documentation, values between 5 and 10 are \"okay\".\n",
    "        # Above 10 is too high and so should be removed.\n",
    "        self.thresh = thresh\n",
    "        \n",
    "        # The statsmodel function will fail with NaN values, as such we have to impute them.\n",
    "        # By default we impute using the median value.\n",
    "        # This imputation could be taken out and added as part of an sklearn Pipeline.\n",
    "        if impute:\n",
    "            self.imputer = SimpleImputer(strategy=impute_strategy)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print('ReduceVIF fit')\n",
    "        if hasattr(self, 'imputer'):\n",
    "            self.imputer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('ReduceVIF transform')\n",
    "        columns = X.columns.tolist()\n",
    "        if hasattr(self, 'imputer'):\n",
    "            X = pd.DataFrame(self.imputer.transform(X), columns=columns)\n",
    "        return ReduceVIF.calculate_vif(X, self.thresh)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_vif(X, thresh=5.0):\n",
    "        # Taken from https://stats.stackexchange.com/a/253620/53565 and modified\n",
    "        dropped=True\n",
    "        while dropped:\n",
    "            variables = X.columns\n",
    "            dropped = False\n",
    "            vif = [variance_inflation_factor(X[variables].values, X.columns.get_loc(var)) for var in X.columns]\n",
    "            \n",
    "            max_vif = max(vif)\n",
    "            if max_vif > thresh:\n",
    "                maxloc = vif.index(max_vif)\n",
    "                print(f'Dropping {X.columns[maxloc]} with vif={max_vif}')\n",
    "                X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n",
    "                dropped=True\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funds = pd.concat([X_train_prepared, y_train_reg.to_frame()], axis=1)\n",
    "features = funds.columns.tolist()\n",
    "target = 'sales_12M_2019'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ReduceVIF()\n",
    "\n",
    "X_tr_vif = transformer.fit_transform(X_train_prepared, y_train_reg)\n",
    "X_tr_vif.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "## Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = feat_pipe.predict(X_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get residuals\n",
    "residuals = y_test_preds - y_test_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions vs residuals\n",
    "fig, axes = plt.subplots(2,2,figsize=(14,10))\n",
    "\n",
    "# plot scatter on upper right plot\n",
    "axes[0,0].scatter(x=y_test_preds, y=residuals, alpha=0.5)\n",
    "axes[0,0].set(xlabel=\"Residuals\",ylabel=\"Predictions\");\n",
    "\n",
    "# plot hist on upper left plot\n",
    "axes[0,1].hist(residuals, bins=50)\n",
    "axes[0,1].set(xlabel='Residuals', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import qqplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes[1,0].set_ylim([-3.5, 3.5])\n",
    "axes[1,0].set_xlim([-3.5, 3.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(residuals, fit=True, line='r', ax=axes[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "## Condition number\n",
    "\n",
    "Numerical analysis has a notion of condition number, which measures how sensitive a function is to changes in the input, and how much error in the output results from an error in the input. In linear regression this number can be used to diagnose multicollinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Find all possible combinations of any length more than 2\n",
    "def all_subsets(set_arg):\n",
    "    return chain(*map(lambda x: combinations(set_arg, x), range(2, len(set_arg)+1)))\n",
    "\n",
    "funds = pd.concat([X_train_prepared, y_train_reg.to_frame()], axis=1)\n",
    "features = funds.columns.tolist() \n",
    "target = 'sales_12M_2019'\n",
    "\n",
    "cond_nums = {}\n",
    "for subset in all_subsets(features):\n",
    "    # checking that target varaible is included in the matrix\n",
    "    if target not in list(subset):\n",
    "        continue\n",
    "    cond_nums[', '.join(list(subset))] = LA.cond(funds[list(subset)])\n",
    "    \n",
    "sorted(cond_nums.items(), key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
